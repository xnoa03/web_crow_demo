import streamlit as st
import pandas as pd
import time
import re
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By


def run_crawler(max_pages):
    status_text = st.empty()
    progress_bar = st.progress(0)
    

    TARGET_TAGS = ['ê²Œì„', 'ê¸°íƒ€', 'ê³µê²œ']
    CLUB_ID = "27646284"
    
    status_text.text("ğŸ“‹ ê²Œì‹œê¸€ ëª©ë¡ì„ ìŠ¤ìº”í•˜ê³  ìˆìŠµë‹ˆë‹¤... (Selenium)")
    
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--blink-settings=imagesEnabled=false")
    chrome_options.page_load_strategy = 'eager'

    driver = webdriver.Chrome(options=chrome_options)
    
    crawl_targets = []
    
    try:
        
        for page in range(1, max_pages + 1):
            status_text.text(f"ğŸ“‹ ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤ìº” ì¤‘... ({page}/{max_pages} í˜ì´ì§€)")
            
            # ì£¼ì†Œ ë’¤ì— &search.page={page} ë¥¼ ë¶™ì—¬ì„œ í˜ì´ì§€ ì´ë™
            target_url = f"https://cafe.naver.com/ArticleList.nhn?search.clubid={CLUB_ID}&search.menuid=44&search.page={page}"
            driver.get(target_url)
            time.sleep(0.5) # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ë„ˆë¬´ ë¹ ë¥´ë©´ ì°¨ë‹¨ë  ìˆ˜ ìˆìŒ)

            posts = driver.find_elements(By.CSS_SELECTOR, "a.article")
            
            # í˜„ì¬ í˜ì´ì§€ì˜ ê¸€ ìˆ˜ì§‘
            for post in posts:
                try:
                    full_title = post.text.strip().replace('\n', ' ')
                    raw_link = post.get_attribute('href')
                    
                    category = "ë¯¸ë¶„ë¥˜"
                    if full_title.startswith("[") and "]" in full_title:
                        end_index = full_title.find("]")
                        category = full_title[1:end_index]
                    
                    if category in TARGET_TAGS and category != "ê³µì§€":
                        match = re.search(r'articles/(\d+)', raw_link)
                        if match:
                            article_id = match.group(1)
                            # ì¤‘ë³µ ìˆ˜ì§‘ ë°©ì§€ (í˜¹ì‹œ ëª°ë¼ì„œ)
                            if not any(d['id'] == article_id for d in crawl_targets):
                                crawl_targets.append({
                                    "id": article_id,
                                    "category": category,
                                    "title": full_title
                                })
                except:
                    continue
            
            # ì§„í–‰ë¥  ë°” ì—…ë°ì´íŠ¸ (ëª©ë¡ ìˆ˜ì§‘ ë‹¨ê³„)
            progress_bar.progress(page / max_pages)
                
    except Exception as e:
        st.error(f"ëª©ë¡ ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}")
    finally:
        driver.quit()

    # ---------------------------------------------------------
    # 2. ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (API í„°ë³´ ëª¨ë“œ)
    # ---------------------------------------------------------
    if not crawl_targets:
        status_text.warning("ìˆ˜ì§‘ ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    status_text.text(f"ğŸš€ ì´ {len(crawl_targets)}ê°œì˜ ê²Œì„ì„ ë°œê²¬! ìƒì„¸ ì •ë³´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤...")
    progress_bar.progress(0) # ë°” ì´ˆê¸°í™”
    
    final_data = []
    
    FORBIDDEN_HEADERS = [
        "1.", "2.", "3.", "4.", "5.", "6.", "7.",
        "ê²Œì„ì´ë¦„", "ì¶œì‹œì¼", "ê°€ê²©", "ë§í¬", "ì£¼ì†Œ", "í•œê¸€", "í”Œë ˆì´íƒ€ì„", "í”Œíƒ€", "ì¶”ì²œì´ìœ "
    ]

    for idx, item in enumerate(crawl_targets):
        # ìƒì„¸ ìˆ˜ì§‘ ì§„í–‰ë¥ 
        progress_bar.progress((idx + 1) / len(crawl_targets))
        
        try:
            api_url = f"https://apis.naver.com/cafe-web/cafe-articleapi/v2.1/cafes/{CLUB_ID}/articles/{item['id']}"
            response = requests.get(api_url)
            data = response.json()
            
            content_html = data['result']['article']['contentHtml']
            soup = BeautifulSoup(content_html, "html.parser")
            content_text = soup.get_text(separator="\n")
            
            info = {
                "ì¹´í…Œê³ ë¦¬": item['category'],
                "ê¸€ì œëª©": item['title'],
                "ê²Œì„ì´ë¦„": "-", "ì¶œì‹œì¼": "-", "ê°€ê²©": "-", 
                "ë§í¬": "-", "í•œê¸€í™”": "-", "í”Œë ˆì´íƒ€ì„": "-",
                "ê²Œì‹œê¸€ë§í¬": f"https://cafe.naver.com/ArticleRead.nhn?clubid={CLUB_ID}&articleid={item['id']}"
            }
            
            if content_text:
                lines = content_text.split('\n')
                
                def get_safe_value(current_idx, all_lines):
                    for k in range(current_idx + 1, len(all_lines)):
                        val = all_lines[k].strip()
                        if val != "":
                            for header in FORBIDDEN_HEADERS:
                                if header in val and len(val) < 30:
                                    return "-"
                            return val
                    return "-"

                for i, line in enumerate(lines):
                    check_line = line.replace(" ", "")
                    
                    if "ê²Œì„ì´ë¦„" in check_line: info["ê²Œì„ì´ë¦„"] = get_safe_value(i, lines)
                    elif "ì¶œì‹œì¼" in check_line: info["ì¶œì‹œì¼"] = get_safe_value(i, lines)
                    elif "ê°€ê²©" in check_line: info["ê°€ê²©"] = get_safe_value(i, lines)
                    elif ("ë§í¬" in check_line or "ì£¼ì†Œ" in check_line) and "http" not in check_line:
                            val = get_safe_value(i, lines)
                            if "http" in val: info["ë§í¬"] = val
                    elif "http" in line and info["ë§í¬"] == "-": info["ë§í¬"] = line.strip()
                    elif "í•œê¸€" in check_line: info["í•œê¸€í™”"] = get_safe_value(i, lines)
                    elif "í”Œë ˆì´íƒ€ì„" in check_line or "í”Œíƒ€" in check_line: info["í”Œë ˆì´íƒ€ì„"] = get_safe_value(i, lines)

            final_data.append(info)
            
        except:
            continue

    status_text.success(f"ğŸ‰ {len(final_data)}ê°œ ê²Œì„ ìˆ˜ì§‘ ì™„ë£Œ! ({max_pages} í˜ì´ì§€)")
    time.sleep(1)
    status_text.empty()
    progress_bar.empty()
    
    return pd.DataFrame(final_data)



st.set_page_config(page_title="í•´ë¦¬ì•¼ í• ë˜ë§ë˜?", page_icon="ğŸ“š", layout="wide")

@st.cache_data
def load_data():
    try:
        df = pd.read_csv('harry_game_list_final.csv')
        if df.empty: return pd.DataFrame()
        return df.fillna('-')
    except:
        return pd.DataFrame()

with st.sidebar:
    st.header("âš™ï¸ ë°ì´í„° ê´€ë¦¬")
    
    # [ì¶”ê°€ë¨] í˜ì´ì§€ ìˆ˜ ì„¤ì • ìŠ¬ë¼ì´ë”
    page_limit = st.number_input("íƒìƒ‰í•  í˜ì´ì§€ ìˆ˜ (1~10)", min_value=1, max_value=20, value=3)
    
    if st.button("ğŸš€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
        with st.spinner(f"ìµœê·¼ {page_limit}í˜ì´ì§€ë¥¼ í›‘ì–´ë³´ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            new_df = run_crawler(page_limit) # í•¨ìˆ˜ì— í˜ì´ì§€ ìˆ˜ ì „ë‹¬
            if not new_df.empty:
                new_df.to_csv('harry_game_list_final.csv', index=False, encoding="utf-8-sig")
                st.cache_data.clear()
                st.rerun() 
            else:
                st.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    st.info(f"ì„¤ì •ëœ í˜ì´ì§€ ìˆ˜ë§Œí¼ ê³¼ê±° ê¸€ê¹Œì§€ íƒìƒ‰í•©ë‹ˆë‹¤.")

st.title("ğŸ“š í•´ë¦¬ì•¼ ì´ ê²Œì„ í• ë˜ë§ë˜?")
st.caption("íŒ¬ì¹´í˜ ì¶”ì²œ ê²Œì„ ë¦¬ìŠ¤íŠ¸ (í˜ì´ì§€ íƒìƒ‰ ê¸°ëŠ¥ ì¶”ê°€)")

df = load_data()

if df.empty:
    st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ í˜ì´ì§€ ìˆ˜ë¥¼ ì •í•˜ê³  [ë°ì´í„° ê°€ì ¸ì˜¤ê¸°] ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!")
else:
    tags = df['ì¹´í…Œê³ ë¦¬'].unique().tolist()
    selected_tags = st.multiselect("ì¥ë¥´ ì„ íƒ", tags, default=tags)
    search_korean = st.text_input("í•œê¸€í™” ê²€ìƒ‰", "")
    
    filtered_df = df[df['ì¹´í…Œê³ ë¦¬'].isin(selected_tags)]
    if search_korean:
        filtered_df = filtered_df[filtered_df['í•œê¸€í™”'].astype(str).str.contains(search_korean, na=False)]
    
    st.dataframe(
        filtered_df,
        use_container_width=True,
        hide_index=True,
        column_order=["ì¹´í…Œê³ ë¦¬", "ê¸€ì œëª©", "ê²Œì„ì´ë¦„", "ê°€ê²©", "í•œê¸€í™”", "í”Œë ˆì´íƒ€ì„", "ë§í¬", "ê²Œì‹œê¸€ë§í¬"],
        column_config={
            "ê¸€ì œëª©": st.column_config.TextColumn("ê²Œì‹œê¸€ ì œëª©", width="medium"),
            "ë§í¬": st.column_config.LinkColumn("ìƒì ", display_text="êµ¬ë§¤ ğŸ”—"),
            "ê²Œì‹œê¸€ë§í¬": st.column_config.LinkColumn("ì›ê¸€", display_text="ì¹´í˜ ğŸ“„"),
            "ê°€ê²©": st.column_config.TextColumn("ê°€ê²©"),
            "í”Œë ˆì´íƒ€ì„": st.column_config.TextColumn("í”Œíƒ€")
        }
    )
    
    st.divider()
    if st.button("ğŸš ë§ˆë²•ì˜ ì†Œë¼ê³ ë™"):
        if not filtered_df.empty:
            pick = filtered_df.sample(1).iloc[0]
            st.balloons()
            st.success(f"### ğŸš€ ì¶”ì²œ: **{pick['ê¸€ì œëª©']}**")
            
            c1, c2, c3, c4 = st.columns(4)
            c1.metric("ì¥ë¥´", pick['ì¹´í…Œê³ ë¦¬'])
            c2.metric("ê²Œì„ëª…", pick['ê²Œì„ì´ë¦„'])
            c3.metric("ê°€ê²©", pick['ê°€ê²©'])
            c4.metric("í•œê¸€í™”", pick['í•œê¸€í™”'])
            st.markdown(f"ğŸ‘‰ [ìƒì  í˜ì´ì§€]({pick['ë§í¬']}) | [ì¶”ì²œê¸€ ë³´ê¸°]({pick['ê²Œì‹œê¸€ë§í¬']})")