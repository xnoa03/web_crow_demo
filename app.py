from flask import Flask, render_template, request, Response, stream_with_context
import pandas as pd
import time
import re
import json
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

app = Flask(__name__)

def generate_crawl_stream(max_pages):
    yield f"data: {json.dumps({'progress': 5, 'msg': 'ğŸš€ ê³ ì† ë¸Œë¼ìš°ì € ëª¨ë“œ ì‹œë™ ì¤‘...'})}\n\n"
    
    TARGET_TAGS = ['ê²Œì„', 'ê¸°íƒ€', 'ê³µê²œ', 'í•  ê²Œì„', 'í• ê²Œì„']
    CLUB_ID = "27646284"
    MENU_ID = "44"
    USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument(f"user-agent={USER_AGENT}")
    chrome_options.add_argument("--blink-settings=imagesEnabled=false")
    chrome_options.page_load_strategy = 'eager' 
    
    driver = webdriver.Chrome(options=chrome_options)
    crawl_targets = []
    
    try:
        for page in range(1, max_pages + 1):
            yield f"data: {json.dumps({'progress': 10 + (page/max_pages*10), 'msg': f'ğŸ“‹ {page}í˜ì´ì§€ ëª©ë¡ ìŠ¤ìº” ì¤‘...'})}\n\n"
            
            target_url = f"https://cafe.naver.com/ArticleList.nhn?search.clubid={CLUB_ID}&search.menuid={MENU_ID}&search.page={page}"
            driver.get(target_url)
            
            try:
                WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, "a.article")))
            except:
                pass

            posts = driver.find_elements(By.CSS_SELECTOR, "a.article")
            for post in posts:
                try:
                    full_title = post.text.strip().replace('\n', ' ')
                    category = "ë¯¸ë¶„ë¥˜"
                    if full_title.startswith("["):
                        end_index = full_title.find("]")
                        if end_index != -1:
                            category = full_title[1:end_index]
                    
                    clean_category = category.replace(" ", "")
                    clean_targets = [t.replace(" ", "") for t in TARGET_TAGS]
                    
                    if clean_category in clean_targets and category != "ê³µì§€":
                        raw_link = post.get_attribute('href')
                        match = re.search(r'articles/(\d+)', raw_link)
                        if match:
                            article_id = match.group(1)
                            if not any(d['id'] == article_id for d in crawl_targets):
                                crawl_targets.append({"id": article_id, "category": category, "title": full_title})
                except:
                    continue
    except Exception as e:
        print(f"ëª©ë¡ ì—ëŸ¬: {e}")
    
    if not crawl_targets:
        driver.quit()
        yield f"data: {json.dumps({'progress': 100, 'msg': 'ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.', 'done': True})}\n\n"
        return

    final_data = []
    FORBIDDEN = ["1.", "2.", "3.", "4.", "5.", "6.", "7.", "ê²Œì„ì´ë¦„", "ì¶œì‹œì¼", "ê°€ê²©", "ë§í¬", "ì£¼ì†Œ", "í•œê¸€", "í•œêµ­ì–´", "ì–¸ì–´", "í”Œë ˆì´íƒ€ì„", "í”Œíƒ€", "ì¶”ì²œì´ìœ "]
    
    total_items = len(crawl_targets)
    yield f"data: {json.dumps({'progress': 25, 'msg': f'ğŸš€ {total_items}ê°œì˜ ë°ì´í„°ë¥¼ ì •ë°€ ë¶„ì„í•©ë‹ˆë‹¤...'})}\n\n"

    for idx, item in enumerate(crawl_targets):
        current_percent = 25 + int((idx + 1) / total_items * 70)
        display_title = item['title'][:15] + "..." if len(item['title']) > 15 else item['title']
        yield f"data: {json.dumps({'progress': current_percent, 'msg': f'[{idx+1}/{total_items}] ë¶„ì„ ì¤‘: {display_title}'})}\n\n"
        
        try:
            api_url = f"https://apis.naver.com/cafe-web/cafe-articleapi/v2.1/cafes/{CLUB_ID}/articles/{item['id']}"
            driver.get(api_url)
            
            try:
                json_elem = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.TAG_NAME, "pre")))
                json_str = json_elem.text
            except:
                json_str = driver.find_element(By.TAG_NAME, "body").text

            data = json.loads(json_str)
            content_html = data['result']['article']['contentHtml']
            soup = BeautifulSoup(content_html, "html.parser")
            content = soup.get_text(separator="\n")
            
            pc_url = f"https://cafe.naver.com/ArticleRead.nhn?clubid={CLUB_ID}&articleid={item['id']}"
            
            info = {
                "ì¹´í…Œê³ ë¦¬": item['category'], "ê¸€ì œëª©": item['title'],
                "ê²Œì„ì´ë¦„": "-", "ì¶œì‹œì¼": "-", "ê°€ê²©": "-", "ë§í¬": "-", "í•œê¸€í™”": "-", "í”Œë ˆì´íƒ€ì„": "-",
                "ê²Œì‹œê¸€ë§í¬": pc_url
            }
            
            if content:
                lines = content.split('\n')
                
                def smart_extract(current_line, current_idx, all_lines, keywords):
                    temp_line = current_line
                    for useless in ["O,X", "O/X", "(O,X)", "(O/X)", "o,x", "o/x", "OX", "ox"]:
                        temp_line = temp_line.replace(useless, "")

                    separators = [":", "-", ")"]
                    for sep in separators:
                        if sep in temp_line:
                            parts = temp_line.split(sep, 1)
                            header_part = parts[0].replace(" ", "")
                            if any(k in header_part for k in keywords):
                                val = parts[1].strip()
                                if "2025/" in val or "2024/" in val: return "-"
                                if ".kr" in val or ".jpg" in val or ".png" in val: return "-"
                                if val: return val
                    
                    cleaned_line = temp_line
                    cleaned_line = re.sub(r'^[0-9]+[\.]?', '', cleaned_line)
                    for k in keywords:
                        cleaned_line = cleaned_line.replace(k, "")
                    cleaned_line = cleaned_line.replace("ì—¬ë¶€", "").strip()
                    
                    if len(cleaned_line) > 0 and len(cleaned_line) < 30:
                        if "2025/" in cleaned_line or ".kr" in cleaned_line: return "-"
                        return cleaned_line

                    for k in range(current_idx + 1, len(all_lines)):
                        val = all_lines[k].strip()
                        if val != "":
                            is_header = False
                            for f in FORBIDDEN:
                                if f in val and len(val) < 30:
                                    is_header = True
                                    break
                            if is_header: return "-"
                            
                            if "2025/" in val or "2024/" in val: return "-"
                            if ".kr" in val and "http" not in val: return "-"
                            
                            return val
                    return "-"

                for i, line in enumerate(lines):
                    l = line.replace(" ", "")
                    
                    if "ê²Œì„ì´ë¦„" in l: 
                        info["ê²Œì„ì´ë¦„"] = smart_extract(line, i, lines, ["ê²Œì„ì´ë¦„"])
                    elif "ì¶œì‹œì¼" in l: 
                        info["ì¶œì‹œì¼"] = smart_extract(line, i, lines, ["ì¶œì‹œì¼", "í•„ìˆ˜ì•„ë‹˜"])
                    elif "ê°€ê²©" in l: 
                        info["ê°€ê²©"] = smart_extract(line, i, lines, ["ê°€ê²©"])
                    elif ("ë§í¬" in l or "ì£¼ì†Œ" in l) and "http" not in l:
                        val = smart_extract(line, i, lines, ["ë§í¬", "ì£¼ì†Œ"])
                        if "http" in val: info["ë§í¬"] = val
                    elif "http" in line and info["ë§í¬"] == "-": 
                        info["ë§í¬"] = line.strip()
                    elif "í•œê¸€" in l or "í•œêµ­ì–´" in l or "ì–¸ì–´" in l or "íŒ¨ì¹˜" in l: 
                        info["í•œê¸€í™”"] = smart_extract(line, i, lines, ["í•œê¸€", "í•œêµ­ì–´", "ì–¸ì–´", "íŒ¨ì¹˜", "í™”", "ì—¬ë¶€"])
                    elif "í”Œë ˆì´íƒ€ì„" in l or "í”Œíƒ€" in l: 
                        info["í”Œë ˆì´íƒ€ì„"] = smart_extract(line, i, lines, ["í”Œë ˆì´íƒ€ì„", "í”Œíƒ€"])

            final_data.append(info)
            
        except Exception as e:
            continue
    
    driver.quit()

    yield f"data: {json.dumps({'progress': 98, 'msg': 'ğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘...'})}\n\n"
    
    if final_data:
        df = pd.DataFrame(final_data)
        df.to_csv('harry_game_list_final.csv', index=False, encoding="utf-8-sig")
    
    yield f"data: {json.dumps({'progress': 100, 'msg': 'ì™„ë£Œ! ìƒˆë¡œê³ ì¹¨í•©ë‹ˆë‹¤.', 'done': True})}\n\n"

def parse_time(text):
    text = str(text).lower()
    if text == '-' or text == '': return 0.0
    nums = re.findall(r"[-+]?\d*\.\d+|\d+", text)
    if not nums: return 0.0
    vals = [float(n) for n in nums]
    avg = sum(vals) / len(vals)
    if ('ë¶„' in text or 'min' in text) and 'ì‹œê°„' not in text: return avg / 60
    return avg

@app.route('/')
def index():
    try:
        df = pd.read_csv('harry_game_list_final.csv').fillna('-')
        df['time_num'] = df['í”Œë ˆì´íƒ€ì„'].apply(parse_time)
        games = df.to_dict(orient='records')
    except:
        games = []
    return render_template('index.html', games=games)

@app.route('/crawl_stream')
def crawl_stream():
    pages = int(request.args.get('pages', 3))
    return Response(stream_with_context(generate_crawl_stream(pages)), mimetype='text/event-stream')

if __name__ == '__main__':
    app.run(debug=True)