import streamlit as st
import pandas as pd
import time
import re
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

# ==========================================
# 1. í¬ë¡¤ë§ ë¡œë´‡ (í´ë¼ìš°ë“œ ìš°íšŒ + í„°ë³´ ëª¨ë“œ)
# ==========================================
def run_crawler(max_pages):
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    # --- [ì„¤ì •] ---
    TARGET_TAGS = ['ê²Œì„', 'ê¸°íƒ€', 'ê³µê²œ']
    CLUB_ID = "27646284" # í•´ì¿ ì•„ë¦¬ì›€
    MENU_ID = "44"       # í•´ë¦¬ì•¼ í• ë˜ë§ë˜ ê²Œì‹œíŒ
    
    # [í•µì‹¬] ë„¤ì´ë²„ê°€ ë´‡ì„ ì°¨ë‹¨í•˜ì§€ ëª»í•˜ê²Œ ì‚¬ëŒì¸ ì²™í•˜ëŠ” ì„¤ì •
    USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    
    status_text.text(f"ğŸ¤– í´ë¼ìš°ë“œ ì„œë²„ì—ì„œ íƒìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
    
    # --------------------------------------------------------
    # 1ë‹¨ê³„: ëª©ë¡ ìˆ˜ì§‘ (Selenium Headless)
    # --------------------------------------------------------
    chrome_options = Options()
    chrome_options.add_argument("--headless") # í™”ë©´ ì—†ì´ ì‹¤í–‰
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080") # í™”ë©´ í¬ê¸° ì„¤ì • (ì¤‘ìš”)
    chrome_options.add_argument(f"user-agent={USER_AGENT}") # ë´‡ íƒì§€ ìš°íšŒ
    
    driver = webdriver.Chrome(options=chrome_options)
    crawl_targets = []
    
    try:
        for page in range(1, max_pages + 1):
            status_text.text(f"ğŸ“‹ {page}í˜ì´ì§€ ìŠ¤ìº” ì¤‘... (ê²Œì‹œê¸€ ëª©ë¡ í™•ë³´)")
            
            target_url = f"https://cafe.naver.com/ArticleList.nhn?search.clubid={CLUB_ID}&search.menuid={MENU_ID}&search.page={page}"
            driver.get(target_url)
            time.sleep(1.5) # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

            posts = driver.find_elements(By.CSS_SELECTOR, "a.article")
            
            # [ë””ë²„ê¹…] ê²Œì‹œê¸€ì„ í•˜ë‚˜ë„ ëª» ì°¾ì•˜ì„ ê²½ìš°
            if len(posts) == 0 and page == 1:
                st.warning(f"âš ï¸ {page}í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë„¤ì´ë²„ ë³´ì•ˆ ì°¨ë‹¨ ê°€ëŠ¥ì„±)")
                # ìŠ¤í¬ë¦°ìƒ· ì°ì–´ì„œ í™•ì¸í•´ë³´ê¸° (ì„ íƒì‚¬í•­)
                # driver.save_screenshot("debug.png")
                # st.image("debug.png")

            for post in posts:
                try:
                    full_title = post.text.strip().replace('\n', ' ')
                    raw_link = post.get_attribute('href')
                    
                    category = "ë¯¸ë¶„ë¥˜"
                    if full_title.startswith("[") and "]" in full_title:
                        end_index = full_title.find("]")
                        category = full_title[1:end_index]
                    
                    if category in TARGET_TAGS and category != "ê³µì§€":
                        match = re.search(r'articles/(\d+)', raw_link)
                        if match:
                            article_id = match.group(1)
                            # ì¤‘ë³µ ë°©ì§€
                            if not any(d['id'] == article_id for d in crawl_targets):
                                crawl_targets.append({
                                    "id": article_id,
                                    "category": category,
                                    "title": full_title
                                })
                except:
                    continue
            
            # í˜ì´ì§€ë³„ ì§„í–‰ë¥  í‘œì‹œ
            progress_bar.progress(page / max_pages * 0.3) # ì „ì²´ ê³µì •ì˜ 30% ë°°ì •
            
    except Exception as e:
        st.error(f"ëª©ë¡ ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}")
    finally:
        driver.quit() # ë¸Œë¼ìš°ì € ì¢…ë£Œ

    # --------------------------------------------------------
    # 2ë‹¨ê³„: ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (API í™œìš© - ê³ ì† ëª¨ë“œ)
    # --------------------------------------------------------
    if not crawl_targets:
        status_text.warning("ìˆ˜ì§‘ ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
        progress_bar.empty()
        return pd.DataFrame()

    status_text.text(f"ğŸš€ {len(crawl_targets)}ê°œì˜ ê²Œì„ì„ ë°œê²¬! ìƒì„¸ ì •ë³´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤...")
    
    final_data = []
    
    # ë°€ë¦¼ ë°©ì§€ë¥¼ ìœ„í•œ 'ë‹¤ìŒ ì§ˆë¬¸(Header)' ê°ì§€ ë¦¬ìŠ¤íŠ¸
    FORBIDDEN_HEADERS = [
        "1.", "2.", "3.", "4.", "5.", "6.", "7.",
        "ê²Œì„ì´ë¦„", "ì¶œì‹œì¼", "ê°€ê²©", "ë§í¬", "ì£¼ì†Œ", "í•œê¸€", "í”Œë ˆì´íƒ€ì„", "í”Œíƒ€", "ì¶”ì²œì´ìœ "
    ]

    for idx, item in enumerate(crawl_targets):
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ë‚˜ë¨¸ì§€ 70%)
        current_progress = 0.3 + ((idx + 1) / len(crawl_targets) * 0.7)
        progress_bar.progress(min(current_progress, 1.0))
        
        try:
            # ë„¤ì´ë²„ ëª¨ë°”ì¼ API ì£¼ì†Œ
            api_url = f"https://apis.naver.com/cafe-web/cafe-articleapi/v2.1/cafes/{CLUB_ID}/articles/{item['id']}"
            
            # [ì¤‘ìš”] API ìš”ì²­ ì‹œì—ë„ User-Agent í—¤ë” í•„ìˆ˜
            headers = {'User-Agent': USER_AGENT}
            response = requests.get(api_url, headers=headers)
            
            if response.status_code != 200: continue

            data = response.json()
            content_html = data['result']['article']['contentHtml']
            
            # HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
            soup = BeautifulSoup(content_html, "html.parser")
            content_text = soup.get_text(separator="\n")
            
            info = {
                "ì¹´í…Œê³ ë¦¬": item['category'],
                "ê¸€ì œëª©": item['title'],
                "ê²Œì„ì´ë¦„": "-", "ì¶œì‹œì¼": "-", "ê°€ê²©": "-", 
                "ë§í¬": "-", "í•œê¸€í™”": "-", "í”Œë ˆì´íƒ€ì„": "-",
                "ê²Œì‹œê¸€ë§í¬": f"https://cafe.naver.com/ArticleRead.nhn?clubid={CLUB_ID}&articleid={item['id']}"
            }
            
            if content_text:
                lines = content_text.split('\n')
                
                # [í•µì‹¬] ì•ˆì „í•˜ê²Œ ê°’ ê°€ì ¸ì˜¤ê¸° (ë°€ë¦¼ ë°©ì§€ ë¡œì§)
                def get_safe_value(current_idx, all_lines):
                    for k in range(current_idx + 1, len(all_lines)):
                        val = all_lines[k].strip()
                        if val != "":
                            # ê°€ì ¸ì˜¨ ì¤„ì´ 'ì§ˆë¬¸(Header)'ì²˜ëŸ¼ ìƒê²¼ìœ¼ë©´ ë°ì´í„° ì—†ìŒ ì²˜ë¦¬
                            for header in FORBIDDEN_HEADERS:
                                if header in val and len(val) < 30:
                                    return "-"
                            return val
                    return "-"

                for i, line in enumerate(lines):
                    check_line = line.replace(" ", "") # ë„ì–´ì“°ê¸° ë¬´ì‹œ ë¹„êµ
                    
                    if "ê²Œì„ì´ë¦„" in check_line: info["ê²Œì„ì´ë¦„"] = get_safe_value(i, lines)
                    elif "ì¶œì‹œì¼" in check_line: info["ì¶œì‹œì¼"] = get_safe_value(i, lines)
                    elif "ê°€ê²©" in check_line: info["ê°€ê²©"] = get_safe_value(i, lines)
                    elif ("ë§í¬" in check_line or "ì£¼ì†Œ" in check_line) and "http" not in check_line:
                            val = get_safe_value(i, lines)
                            if "http" in val: info["ë§í¬"] = val
                    elif "http" in line and info["ë§í¬"] == "-": # ë³¸ë¬¸ì— ë©ê·¸ëŸ¬ë‹ˆ ìˆëŠ” ë§í¬
                         info["ë§í¬"] = line.strip()
                    elif "í•œê¸€" in check_line: info["í•œê¸€í™”"] = get_safe_value(i, lines)
                    elif "í”Œë ˆì´íƒ€ì„" in check_line or "í”Œíƒ€" in check_line: info["í”Œë ˆì´íƒ€ì„"] = get_safe_value(i, lines)

            final_data.append(info)
        except:
            continue

    status_text.success(f"ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ! ({len(final_data)}ê°œ)")
    time.sleep(1)
    status_text.empty()
    progress_bar.empty()
    
    return pd.DataFrame(final_data)


# ==========================================
# 2. ì›¹ UI êµ¬ì„±
# ==========================================
st.set_page_config(page_title="í•´ë¦¬ì•¼ í• ë˜ë§ë˜?", page_icon="ğŸ¦¦", layout="wide")

@st.cache_data
def load_data():
    try:
        df = pd.read_csv('harry_game_list_final.csv')
        if df.empty: return pd.DataFrame()
        return df.fillna('-')
    except:
        return pd.DataFrame()

with st.sidebar:
    st.header("âš™ï¸ ë°ì´í„° ê´€ë¦¬")
    
    # í˜ì´ì§€ ìˆ˜ ì„¤ì •
    page_limit = st.number_input("íƒìƒ‰í•  í˜ì´ì§€ ìˆ˜ (1~10)", min_value=1, max_value=10, value=3)
    
    if st.button("ğŸš€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
        with st.spinner("í•´ì¿ ì•„ë¦¬ì›€ì— ì ‘ì† ì¤‘ì…ë‹ˆë‹¤..."):
            new_df = run_crawler(page_limit)
            if not new_df.empty:
                new_df.to_csv('harry_game_list_final.csv', index=False, encoding="utf-8-sig")
                st.cache_data.clear()
                st.rerun() 
            else:
                st.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”)")
    
    st.info("ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¹´í˜ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤.")

st.title("ğŸ¦¦ í•´ë¦¬ì•¼ ì´ ê²Œì„ í• ë˜ë§ë˜?")
st.caption("íŒ¬ì¹´í˜ ì¶”ì²œ ê²Œì„ ë¦¬ìŠ¤íŠ¸ (í´ë¼ìš°ë“œ ìµœì í™” ë²„ì „)")

df = load_data()

if df.empty:
    st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ [ë°ì´í„° ê°€ì ¸ì˜¤ê¸°] ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!")
else:
    # í•„í„°ë§
    tags = df['ì¹´í…Œê³ ë¦¬'].unique().tolist()
    selected_tags = st.multiselect("ì¥ë¥´ ì„ íƒ", tags, default=tags)
    search_korean = st.text_input("í•œê¸€í™” ê²€ìƒ‰", "")
    
    filtered_df = df[df['ì¹´í…Œê³ ë¦¬'].isin(selected_tags)]
    if search_korean:
        filtered_df = filtered_df[filtered_df['í•œê¸€í™”'].astype(str).str.contains(search_korean, na=False)]
    
    # í…Œì´ë¸” ì¶œë ¥
    st.dataframe(
        filtered_df,
        use_container_width=True,
        hide_index=True,
        column_order=["ì¹´í…Œê³ ë¦¬", "ê¸€ì œëª©", "ê²Œì„ì´ë¦„", "ê°€ê²©", "í•œê¸€í™”", "í”Œë ˆì´íƒ€ì„", "ë§í¬", "ê²Œì‹œê¸€ë§í¬"],
        column_config={
            "ê¸€ì œëª©": st.column_config.TextColumn("ê²Œì‹œê¸€ ì œëª©", width="medium"),
            "ë§í¬": st.column_config.LinkColumn("ìƒì ", display_text="êµ¬ë§¤ ğŸ”—"),
            "ê²Œì‹œê¸€ë§í¬": st.column_config.LinkColumn("ì›ê¸€", display_text="ì¹´í˜ ğŸ“„"),
            "ê°€ê²©": st.column_config.TextColumn("ê°€ê²©"),
            "í”Œë ˆì´íƒ€ì„": st.column_config.TextColumn("í”Œíƒ€")
        }
    )
    
    st.divider()
    
    # ëœë¤ ì¶”ì²œ
    if st.button("ğŸš ë§ˆë²•ì˜ ì†Œë¼ê³ ë™"):
        if not filtered_df.empty:
            pick = filtered_df.sample(1).iloc[0]
            st.balloons()
            
            # ì¶”ì²œ ë©”ì‹œì§€ (ê²Œì„ ì´ë¦„ì´ ì—†ìœ¼ë©´ ê¸€ ì œëª© ì‚¬ìš©)
            display_title = pick['ê²Œì„ì´ë¦„'] if pick['ê²Œì„ì´ë¦„'] != '-' else pick['ê¸€ì œëª©']
            
            st.success(f"### ğŸš€ ì¶”ì²œ: **{display_title}**")
            
            c1, c2, c3, c4 = st.columns(4)
            c1.metric("ì¥ë¥´", pick['ì¹´í…Œê³ ë¦¬'])
            c2.metric("ê°€ê²©", pick['ê°€ê²©'])
            c3.metric("í•œê¸€í™”", pick['í•œê¸€í™”'])
            c4.metric("í”Œíƒ€", pick['í”Œë ˆì´íƒ€ì„'])
            
            st.markdown(f"ğŸ‘‰ [ìƒì  í˜ì´ì§€ ë°”ë¡œê°€ê¸°]({pick['ë§í¬']})")
            st.markdown(f"ğŸ‘‰ [ì¶”ì²œê¸€ ë³´ëŸ¬ê°€ê¸°]({pick['ê²Œì‹œê¸€ë§í¬']})")
        else:
            st.warning("ì¶”ì²œí•  ê²Œì„ì´ ì—†ìŠµë‹ˆë‹¤.")
