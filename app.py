import streamlit as st
import pandas as pd
import time
import re
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

def run_crawler(max_pages):
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    TARGET_TAGS = ['ê²Œì„', 'ê¸°íƒ€', 'ê³µê²œ','í•  ê²Œì„']
    CLUB_ID = "27646284"
    MENU_ID = "44"
    
    USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    
    status_text.text(f"í´ë¼ìš°ë“œ ì„œë²„ì—ì„œ íƒìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
    
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument(f"user-agent={USER_AGENT}")
    
    driver = webdriver.Chrome(options=chrome_options)
    crawl_targets = []
    
    try:
        for page in range(1, max_pages + 1):
            status_text.text(f"{page}í˜ì´ì§€ ìŠ¤ìº” ì¤‘...")
            
            target_url = f"https://cafe.naver.com/ArticleList.nhn?search.clubid={CLUB_ID}&search.menuid={MENU_ID}&search.page={page}"
            driver.get(target_url)
            time.sleep(1.5)

            posts = driver.find_elements(By.CSS_SELECTOR, "a.article")
            
            for post in posts:
                try:
                    full_title = post.text.strip().replace('\n', ' ')
                    raw_link = post.get_attribute('href')
                    
                    category = "ë¯¸ë¶„ë¥˜"
                    if full_title.startswith("[") and "]" in full_title:
                        end_index = full_title.find("]")
                        category = full_title[1:end_index]
                    
                    if category in TARGET_TAGS and category != "ê³µì§€":
                        match = re.search(r'articles/(\d+)', raw_link)
                        if match:
                            article_id = match.group(1)
                            if not any(d['id'] == article_id for d in crawl_targets):
                                crawl_targets.append({
                                    "id": article_id,
                                    "category": category,
                                    "title": full_title
                                })
                except:
                    continue
            
            progress_bar.progress(page / max_pages * 0.3)
            
    except Exception as e:
        st.error(f"ëª©ë¡ ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}")
    finally:
        driver.quit()

    if not crawl_targets:
        status_text.warning("ìˆ˜ì§‘ ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
        progress_bar.empty()
        return pd.DataFrame()

    status_text.text(f"{len(crawl_targets)}ê°œì˜ ê²Œì„ì„ ë°œê²¬! ìƒì„¸ ì •ë³´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤...")
    
    final_data = []
    
    FORBIDDEN_HEADERS = [
        "1.", "2.", "3.", "4.", "5.", "6.", "7.",
        "ê²Œì„ì´ë¦„", "ì¶œì‹œì¼", "ê°€ê²©", "ë§í¬", "ì£¼ì†Œ", "í•œê¸€", "í”Œë ˆì´íƒ€ì„", "í”Œíƒ€", "ì¶”ì²œì´ìœ "
    ]

    for idx, item in enumerate(crawl_targets):
        current_progress = 0.3 + ((idx + 1) / len(crawl_targets) * 0.7)
        progress_bar.progress(min(current_progress, 1.0))
        
        try:
            api_url = f"https://apis.naver.com/cafe-web/cafe-articleapi/v2.1/cafes/{CLUB_ID}/articles/{item['id']}"
            headers = {'User-Agent': USER_AGENT}
            response = requests.get(api_url, headers=headers)
            
            if response.status_code != 200: continue

            data = response.json()
            content_html = data['result']['article']['contentHtml']
            soup = BeautifulSoup(content_html, "html.parser")
            content_text = soup.get_text(separator="\n")
            
            info = {
                "ì¹´í…Œê³ ë¦¬": item['category'],
                "ê¸€ì œëª©": item['title'],
                "ê²Œì„ì´ë¦„": "-", "ì¶œì‹œì¼": "-", "ê°€ê²©": "-", 
                "ë§í¬": "-", "í•œê¸€í™”": "-", "í”Œë ˆì´íƒ€ì„": "-",
                "ê²Œì‹œê¸€ë§í¬": f"https://cafe.naver.com/ArticleRead.nhn?clubid={CLUB_ID}&articleid={item['id']}"
            }
            
            if content_text:
                lines = content_text.split('\n')
                
                def get_safe_value(current_idx, all_lines):
                    for k in range(current_idx + 1, len(all_lines)):
                        val = all_lines[k].strip()
                        if val != "":
                            for header in FORBIDDEN_HEADERS:
                                if header in val and len(val) < 30:
                                    return "-"
                            return val
                    return "-"

                for i, line in enumerate(lines):
                    check_line = line.replace(" ", "")
                    
                    if "ê²Œì„ì´ë¦„" in check_line: info["ê²Œì„ì´ë¦„"] = get_safe_value(i, lines)
                    elif "ì¶œì‹œì¼" in check_line: info["ì¶œì‹œì¼"] = get_safe_value(i, lines)
                    elif "ê°€ê²©" in check_line: info["ê°€ê²©"] = get_safe_value(i, lines)
                    elif ("ë§í¬" in check_line or "ì£¼ì†Œ" in check_line) and "http" not in check_line:
                            val = get_safe_value(i, lines)
                            if "http" in val: info["ë§í¬"] = val
                    elif "http" in line and info["ë§í¬"] == "-": 
                         info["ë§í¬"] = line.strip()
                    elif "í•œê¸€" in check_line: info["í•œê¸€í™”"] = get_safe_value(i, lines)
                    elif "í”Œë ˆì´íƒ€ì„" in check_line or "í”Œíƒ€" in check_line: info["í”Œë ˆì´íƒ€ì„"] = get_safe_value(i, lines)

            final_data.append(info)
        except:
            continue

    status_text.success(f"ìˆ˜ì§‘ ì™„ë£Œ! ({len(final_data)}ê°œ)")
    time.sleep(1)
    status_text.empty()
    progress_bar.empty()
    
    return pd.DataFrame(final_data)

st.set_page_config(page_title="í•´ë¦¬ì•¼ í• ë˜ë§ë˜?", page_icon="ğŸ¦¦", layout="wide")

def parse_playtime_to_hours(text):
    text = str(text).lower()
    if text == '-' or text == '':
        return 0.0
    
    numbers = re.findall(r"[-+]?\d*\.\d+|\d+", text)
    if not numbers:
        return 0.0
    
    values = [float(n) for n in numbers]
    avg_value = sum(values) / len(values)
    
    if 'ë¶„' in text or 'min' in text:
        if 'ì‹œê°„' not in text and 'hour' not in text:
            return avg_value / 60
            
    return avg_value

@st.cache_data
def load_data():
    try:
        df = pd.read_csv('harry_game_list_final.csv')
        if df.empty: return pd.DataFrame()
        df = df.fillna('-')
        df['playtime_hours'] = df['í”Œë ˆì´íƒ€ì„'].apply(parse_playtime_to_hours)
        return df
    except:
        return pd.DataFrame()

with st.sidebar:
    st.header("âš™ï¸ ë°ì´í„° ê´€ë¦¬")
    
    page_limit = st.number_input("íƒìƒ‰í•  í˜ì´ì§€ ìˆ˜ (1~10)", min_value=1, max_value=10, value=3)
    
    if st.button("ğŸš€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
        with st.spinner("í•´ì¿ ì•„ë¦¬ì›€ì— ì ‘ì† ì¤‘ì…ë‹ˆë‹¤..."):
            new_df = run_crawler(page_limit)
            if not new_df.empty:
                new_df.to_csv('harry_game_list_final.csv', index=False, encoding="utf-8-sig")
                st.cache_data.clear()
                st.rerun() 
            else:
                st.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    st.info("ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¹´í˜ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤.")

st.title("ğŸ¦¦ í•´ë¦¬ì•¼ í• ë˜ë§ë˜?")
st.caption("íŒ¬ì¹´í˜ ì¶”ì²œ ê²Œì„ ë¦¬ìŠ¤íŠ¸")

df = load_data()

if df.empty:
    st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ [ë°ì´í„° ê°€ì ¸ì˜¤ê¸°] ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!")
else:
    tags = df['ì¹´í…Œê³ ë¦¬'].unique().tolist()
    selected_tags = st.multiselect("ì¥ë¥´ ì„ íƒ", tags, default=tags)
    
    max_time_in_data = int(df['playtime_hours'].max()) + 1 if not df.empty else 100
    
    time_filter = st.slider(
        "ìµœëŒ€ í”Œë ˆì´íƒ€ì„ (ì‹œê°„)", 
        min_value=0, 
        max_value=max_time_in_data, 
        value=max_time_in_data
    )
    
    filtered_df = df[
        (df['ì¹´í…Œê³ ë¦¬'].isin(selected_tags)) &
        (df['playtime_hours'] <= time_filter)
    ]
    
    st.dataframe(
        filtered_df,
        use_container_width=True,
        hide_index=True,
        column_order=["ì¹´í…Œê³ ë¦¬", "ê¸€ì œëª©", "ê²Œì„ì´ë¦„", "ê°€ê²©", "í•œê¸€í™”", "í”Œë ˆì´íƒ€ì„", "ë§í¬", "ê²Œì‹œê¸€ë§í¬"],
        column_config={
            "ê¸€ì œëª©": st.column_config.TextColumn("ê²Œì‹œê¸€ ì œëª©", width="medium"),
            "ë§í¬": st.column_config.LinkColumn("ìƒì ", display_text="êµ¬ë§¤ ğŸ”—"),
            "ê²Œì‹œê¸€ë§í¬": st.column_config.LinkColumn("ì›ê¸€", display_text="ì¹´í˜ ğŸ“„"),
            "ê°€ê²©": st.column_config.TextColumn("ê°€ê²©"),
            "í”Œë ˆì´íƒ€ì„": st.column_config.TextColumn("í”Œíƒ€")
        }
    )
    
    st.divider()
    
    if st.button("ëœë¤ ê²Œì„ Picker"):
        if not filtered_df.empty:
            pick = filtered_df.sample(1).iloc[0]
            st.balloons()
            
            display_title = pick['ê²Œì„ì´ë¦„'] if pick['ê²Œì„ì´ë¦„'] != '-' else pick['ê¸€ì œëª©']
            
            st.success(f"### ğŸš€ ì¶”ì²œ: **{display_title}**")
            
            c1, c2, c3, c4 = st.columns(4)
            c1.metric("ì¥ë¥´", pick['ì¹´í…Œê³ ë¦¬'])
            c2.metric("ê°€ê²©", pick['ê°€ê²©'])
            c3.metric("í•œê¸€í™”", pick['í•œê¸€í™”'])
            c4.metric("í”Œíƒ€", pick['í”Œë ˆì´íƒ€ì„'])
            
            st.markdown(f"ğŸ‘‰ [ìƒì  í˜ì´ì§€ ë°”ë¡œê°€ê¸°]({pick['ë§í¬']})")
            st.markdown(f"ğŸ‘‰ [ì¶”ì²œê¸€ ë³´ëŸ¬ê°€ê¸°]({pick['ê²Œì‹œê¸€ë§í¬']})")
        else:
            st.warning("ì¡°ê±´ì— ë§ëŠ” ê²Œì„ì´ ì—†ìŠµë‹ˆë‹¤.")